##### 使用随机森林实现mnist数据集的分类
##### 该数据集共1797个样本，64维特征，使用随机森林包含100个决策树

##### 决策树和随机森林的区别

- 决策树基于给定的数据来产生规则，随机森林随机从特征集合中选择一些来构建决策树
- 随机森林可以改进决策树过拟合问题
- 决策树计算更快

##### 随机森林训练过程

1. 从数据集$X, Y$中通过自助法(有放回的采样)采样$n$ 个训练样本组成子集$X_b,Y_b$。
2. 从所有的特征中随机选择$N$ 个特征。
3. 选择最优化分属性。
4. 生成子节点。
5. 重复前面的3步直到生成$l$个节点。
6. 重复前面的4步直到产生$B$个树。

##### 测试阶段

回归树输出 $\hat f=\frac{1}{B}\sum_{b=1}^Bf_b(x)$

分类树采用投票方法。

##### 构建随机森林采用sklearn.ensemble中的RandomForestClassifier类，该类主要参数有

- **n_estimators:**  决策树的数量，决策树越多，结果越稳定可靠，同时消耗资源越多，在0.20版中默认是10，0.22版中默认100
- **criterion:**  评估划分质量，默认是**Gini**
- **max_depth:**  默认是None
- **max_features: ** 每次划分时选择的特征数量，默认是auto，$\sqrt {number\_of\_features}$
- **min_samples_leaf:** 默认1
- **n_jobs:** 线程数，默认1
- **oob_score:**  是否使用out-of-bag采样提高精度，默认是False

##### 随机森林的优缺点

1. 优点
   - 随机森林是一个鲁棒和通用的算法
   - 可以处理有缺失值的数据
   - 可以解决无监督的机器学习问题
   - 可以解决过拟合问题
   - 可以作为一个特征选择工具（特征重要性）
   - 可以处理高维数据
2. 缺点
   - 计算成本高
   - 很难解释
   - 耗时
   - 预测往往比较慢